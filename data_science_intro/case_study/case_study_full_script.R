# ----------------------------------------------
# CASE STUDY - MACHINE LEARNING IN R
# ----------------------------------------------
# In this section, we'll read in the data, and take a look at it. 

#' ## Regression	
#' Within the world of supervised learning, we can divide tasks into two parts. In settings where the response variable is continuous we call the modelling *regression*, and when the response is categorical we call it *classification*. We will begin with regression to understand what factors influence price in the AirBnB data set.	

#' Let's start by loading the data (after first setting the correct working directory). We'll use the 'listings.csv' file for now. Since we'll be wrangling and visualizing, we'll also load the `tidyverse` package. (Instead of `tidyverse`, it also works to load `tidyr`, `dplyr`, and `ggplot2` as we saw last session.)	


listings <-read.csv("../data/listings.csv",stringsAsFactors = FALSE)	
library(tidyverse)	

#' As a review, we need to change the price column to a numeric form.

listings <-listings %>% mutate(price = as.numeric(gsub("\\$|,","",price)))	
summary(listings$price) # Check to make sure things worked	


#' Now, which variables may be predictive of price? We can use `names(listings)` to get a look at all the variable names.	

#' #### Data Preparation	
# ----------------------------------------------
# EXERCISE 1: LOOK AT DATA
# ----------------------------------------------

# ----------------------------------------------
# SOLUTION
# ----------------------------------------------
#' Let's begin by looking at the relationship between `listings$accommodates` and `listings$price`. As a first look:	
listings %>%
  ggplot() +
  aes(x=accommodates, y=price) +
  geom_point()

#' Looks like there are some outliers on both axes. There are fancier ways to deal with this statistically, but for today let's just get rid of the outliers and fit a model on the cleaner data:	

listings_for_lm <-listings %>%	
  filter(accommodates <= 10, price <=1000)	

#' Let's take another look:	
listings_for_lm %>%
  ggplot() + 
  aes(x=accommodates, y=price) +
  geom_point() 


#' let's try adding some other variables
listings_for_lm %>%
  ggplot() +
  aes(x=accommodates, y=price, color=cancellation_policy, size=review_scores_rating) +
  geom_point(alpha = 0.2) +
  facet_wrap(~cancellation_policy,ncol=4)

# ---- end of example solutions to Exercise 1 -----

# -----------------------------------------------------------------
# Linear Regression
# -----------------------------------------------------------------

#' You can add a linear fit to a scatter plot easily by adding geom_smooth with method="lm"
listings %>%
  ggplot() +
  aes(x=accommodates, y=price) +
  geom_point() +
  geom_smooth(method="lm")

#' What if we need to quantify the linear fit, check it statistical significance, extend to multiple variables, and make predictions? A more rigorous statistical procedure is therefore needed.

#' Since we care about prediction accuracy, we'll reserve a portion of our data to be a test set. There are lots of ways to do this. We'll use the `modelr` package, which is part of the `tidyverse`.	


library(modelr) # Comes with tidyverse installation, but doesn't automatically load with the library(tidyverse) call	
set.seed(1)  # Keep the random partition the same for multiple runs
listings_part <-listings_for_lm %>% 	
  resample_partition(c(train=0.7,test=0.3))	
listings_part

#' The object `listings_for_lm` is now a list with two elements: `train` and `test`.	

#' #### Model Fitting	
#' In R, we specify a model structure and then use the corresponding function to tell R to optimize for the best-fitting model. For linear regression, the function is `lm()`:	

lm_price_by_acc <-lm(price ~ accommodates,data=listings_part$train) # We'll talk more about the '~' notation soon	


#' Let's check out the lm_price_by_acc object:	

names(lm_price_by_acc)	


#' The object `lm_price_by_acc` is a list of a bunch of relevant information generated by the `lm()` function call. We can use the `$` to view different elements, for example	

lm_price_by_acc$call	


#' So, it stores the function call in one of the list elements. But this isn't the most useful way to check out the model fit. The function `summary()` is overloaded for many different objects and often gives a useful snapshot of the model:	

summary(lm_price_by_acc)	

#' There we go, this is more useful! First, let's look at the section under "Coefficients". Notice that R automatically adds an intercept term unless you tell it not to (we'll see how to do this later). In the "estimate" column, we see that the point estimates for the linear model here say that the price is \$55.20 plus \$37.79 for every person accommodated. Notice the '***' symbols at the end of the "(Intercept)" and "accommodates" rows. These indicate that according to a statistical t-test, both coefficients are extremely significantly different than zero, so things are okay from an inference perspective.	



# -----------------------------------------------------------------
# Model evaluation
# -----------------------------------------------------------------
# ----------------------------------------------
# EXERCISE 2: VISUALIZE MODEL PERFORMANCE
# ----------------------------------------------

# There are some nifty functions in the `modelr` package that make interacting with models easy in the `tidyr` and `dplyr` setting. We'll use `modelr::add_predictions()` here. We can also remove the linear trend and check the residual uncertainty, which we'll do here using `modelr::add_residuals()`. 	

listing_with_pred <-  as.data.frame(listings_part$train) %>%	
  add_predictions(lm_price_by_acc) %>%	
  add_residuals(lm_price_by_acc,var="resid") 

# ----------------------------------------------
# SOLUTION
# ----------------------------------------------
#' As a check on inference quality, let's plot the fitted line. 
listing_with_pred %>%
  ggplot() +	
  aes(x=accommodates) + 
  geom_point(aes(y=price)) +	
  geom_line(aes(y=pred), color='red')	


#' Nice. This is helpful to make sure that the residual uncertainty looks like random noise rather than an unidentified trend.	

listing_with_pred %>%	
  ggplot() + 
  aes(x=accommodates,y=resid) + 
  geom_point()	


#' Since we have finitely many values, maybe box plots tell a better story:	

listing_with_pred %>%	
  group_by(as.factor(accommodates)) %>%	
  ggplot() + 
  aes(x=as.factor(accommodates),y=resid) + 
  geom_boxplot()

#' Things are pretty centered around zero, with the exception of 9- & 10-person accommodations. Maybe the model doesn't apply so well here and we could refine it in a later modelling iteration.	

#' Let's now take a look at out-of-sample performance. We'll plot the predictions versus the actuals as we did before, but this time for the test data.	

as.data.frame(listings_part$test) %>%	
  add_predictions(lm_price_by_acc) %>%	
  ggplot() +	
  aes(x=accommodates) + 
  geom_point(aes(y=price)) +	
  geom_line(aes(y=pred), color='red')	

# ---- end of example solutions to Exercise 2 -----
 
#' Now, what if we wanted to *quantify* how well the model predicts these out-of-sample values? There are several metrics to aggregate the prediction error. We'll look at RMSE, MAE, and Rsquared.

#' You could do this pretty easily by hand, and we'll do so later on, but `modelr` has built-in functions to do this for you:	

rmse(lm_price_by_acc,listings_part$test)	
mae(lm_price_by_acc,listings_part$test)	
rsquare(lm_price_by_acc,listings_part$test)	

#' These don't help much on their own, but they come in handy when comparing different models, which we'll do next after a quick review.	

#' We interacted with the model to evaluate goodness-of-fit and out-of-sample performance. In our case, we used the `modelr` and `dplyr` framework to do this cleanly.	

# -----------------------------------------------------------------
# Adding More Variables
# -----------------------------------------------------------------
#' Let's work a bit harder on predicting price, this time using more than one predictor. In fact, we'll add a bunch of predictors to the model and see what happens.	

#' As one set of predictors, the column listings$amenities looks interesting:	

listings %>% select(amenities) %>% head()	

#' This could be good predictive information if we can separate out which listing has which amenity. Our goal here is to turn the amenities column into many columns, one for each amenity, and with logical values indicating whether each listing has each amenity. This is just a bit tricky with some text manipulation. You can just include the code below and trust me on it.

listings <-listings %>%
  filter(!grepl("translation missing",amenities)) %>%
  mutate(amenities=gsub("[{}]|\"|[()]|-","",amenities)) %>%
  mutate(amenities=gsub(" |/","_",amenities)) %>%
  mutate(amenities=gsub("24","x24",amenities))

# Then, split the strings by amenity and create new column
splitting <-strsplit(listings$amenities,",")
all_amenities <-Reduce(union,splitting)
for (i in all_amenities){
  listings[paste("amenity_",i,sep="")] <-grepl(i,listings$amenities)
}

#' Before building the model, let's clean up the data by getting rid of missing values and outliers. For categorical variables, we will remove all categories with only a few observations. Finally, we'll separate again into training and test sets.	

listings_big <- listings %>%	
  filter(!is.na(review_scores_rating),	
         accommodates <= 10,	
         property_type %in% c("Apartment","House","Bed & Breakfast","Condominium","Loft","Townhouse"),	
         !(neighbourhood_cleansed %in% c("Leather District","Longwood Medical Area")),	
         price <= 1000) %>%	
  select(price,accommodates,room_type,property_type,review_scores_rating,neighbourhood_cleansed,starts_with("amenity"))	

#' note the use `starts_with` as a handy shortcut to select groups of variables

# ----------------------------------------------
# EXERCISE 3: BUILD NEW LINEAR MODELS
# ----------------------------------------------
# ----------------------------------------------
# SOLUTION
# ----------------------------------------------
#' In total, we'll use all of these predictors:	

#' * accommodates	
#' * property_type	
#' * review_scores_rating	
#' * neighbourhood_cleansed	
#' * accommodates*room_type	
#' * property_type*neighbourhood_cleansed 	
#' * review_scores_rating*neighbourhood_cleansed 	
#' * accommodates*review_scores_rating	
#' * All columns created from the amenities column	

#' Note that whenever we include a non-numeric (or categorical) variable, R is going to create one indicator variable for all but one unique value of the variable. We'll see this in the output of `lm()`.	

#' To get R to learn the model, we need to pass it a formula. We don't want to write down all those amenity variables by hand. Luckily, we can use the `paste()` function to string all the variable names together, and then the `as.formula()` function to translate a string into a formula.	

all_amenities <- as.data.frame(listings_big_lm$train) %>% select(starts_with("amenity")) %>% names()	
amenities_string <- paste(all_amenities,collapse="+")	
amenities_string # Taking a look to make sure things worked	


big_formula <- as.formula(paste("price ~ accommodates + accommodates*room_type + property_type + neighbourhood_cleansed + property_type*neighbourhood_cleansed + review_scores_rating*neighbourhood_cleansed + accommodates*review_scores_rating",amenities_string,sep="+"))	


#' Don't forget to split the data before running `lm()`:

listings_big_lm <- listings_big %>%	
  resample_partition(c(train=0.7,test=0.3))	


#' Now we can use the `lm()` function:	

big_price_lm <- lm(big_formula,data=listings_big_lm$train)	

#' We won't look at the summary because there are so many predictors. What happens when we compare in-sample and out-of-sample prediction performance?	

rmse(big_price_lm,listings_big_lm$train) # In-sample	
rmse(big_price_lm,listings_big_lm$test) # Out-of-sample	

#' We've got an overfitting problem here, meaning that the training error is smaller than the test error. The model is too powerful for the amount of data we have. Note that R recognizes this by giving warnings about a "rank-deficient fit."	

# ---- end of example solutions to Exercise 3 -----

# -----------------------------------------------------------------
# LASSO (Penalized regression)
# -----------------------------------------------------------------
#' But is there still a way to use the info from all these variables without overfitting? Yes! One way to do this is by regularized, or penalized, regression.	

#' To do LASSO, we'll use the `glmnet` package. Of note, this package doesn't work very elegantly with the `tidyverse` since it uses matrix representations of the data rather than data frame representations. However, it does what it does quite well, and will give us a chance to see some base R code. Let's load the package and check out the function `glmnet()`. We can see the documentation from the command line using `?glmnet`.	

library(glmnet)	


#' Notice that `glmnet()` doesn't communicate with the data via formulas. Instead, it wants a matrix of predictor variables and a vector of values for the variable we're trying to predict, including all the categorical variables that R automatically expanded into indicator variables. Fortunately, R has a `model.matrix()` function which takes a data frame and gets it into the right form for `glmnet()` and other functions with this type of input.	

#' Notice also that there's a way to specify lambda manually. Since we haven't discussed choosing lambda yet, let's just accept the default for now and see what we get.	


x <- model.matrix(~ .-price + accommodates*room_type + property_type*neighbourhood_cleansed + review_scores_rating*neighbourhood_cleansed + accommodates*review_scores_rating,data=as.data.frame(listings_big_lm$train))	
y <- as.vector(as.data.frame(listings_big_lm$train)$price)	
lasso_price <- glmnet(x,y)	


#' This time the `summary()` function isn't quite as useful:	

summary(lasso_price)	


#' It does give us some info, though. Notice that "lambda" is a vector of length 100. The `glmnet()` function has defined 100 different values of lambda and found the corresponding optimal beta vector for each one! We have 100 different models here. Let's look at some of the coefficients for the different models. We'll start with one where lambda is really high:	

lasso_price$lambda[1]	
nnzero(lasso_price$beta[,1]) # How many coefficients are nonzero?	


#' Here the penalty on the size of the coefficients is so high that R sets them all to zero. Moving to some smaller lambdas:	

lasso_price$lambda[10]	
lasso_price$beta[which(lasso_price$beta[,10] != 0),10]	
	
lasso_price$lambda[20]	
lasso_price$beta[which(lasso_price$beta[,20] != 0),20]	


#' And, to see the whole path of lambdas:	

plot.glmnet(lasso_price,xvar="lambda")	


#' Here, each line is one variable. The plot is quite messy with so many variables, but it gives us the idea. As lambda shrinks, the model adds more and more nonzero coefficients.	

# -----------------------------------------------------------------
# Cross Validation
# -----------------------------------------------------------------
#' How do we choose which of the 88 models to use? Or in other words, how do we "tune" the $\lambda$ parameter? We'll use a similar idea to the training-test set split called cross-validation.	

#' Let's do the cross-validation:	
set.seed(1)
lasso_price_cv <- cv.glmnet(x,y)	
summary(lasso_price_cv) # What does the model object look like?	
lasso_price_cv$lambda.min

#' Notice the "lambda.min". This is the best lambda as determined by the cross validation. "lambda.1se" is the largest lambda such that the "error is within 1 standard error of the minimum."	

#' There's another automatic plotting function for `cv.glmnet()` which shows the error for each model:	

plot.cv.glmnet(lasso_price_cv)	

#' The first vertical dotted line shows `lambda.min`, and the second is `lambda.1se`. The figure illustrates that we cross-validate to find the "sweet spot" where there's not too much bias (high lambda) and not too much noise (low lambda).

#' Let's again compare training and test error. Since the `predict()` function for `glmnet` objects uses matrices, we can't use the `rmse` function like we did before.	
x_all <- model.matrix(~ .-price + accommodates*room_type + property_type*neighbourhood_cleansed + review_scores_rating*neighbourhood_cleansed + accommodates*review_scores_rating,data=listings_big) # Matrix form for combined test and training data	
	
lasso.pred <- predict.cv.glmnet(lasso_price_cv,newx=x_all, s="lambda.min")

listings_big %>%	
  mutate(is_test = 1:nrow(listings_big) %in% listings_big_lm$test$idx,	
         pred = lasso.pred) %>%	
  group_by(is_test) %>%	
  summarize(rmse <- sqrt(1/length(price)*sum((price-pred)^2)))	

#' The overfitting problem has gotten better, but hasn't yet gone away completely. I added a bunch variables for dramatic effect that we could probably screen out before running the LASSO if we really wanted a good model.	

#' One more note on cross-validation: the `glmnet` package has built-in functionality for cross-validation. In situations where that's not the case, `modelr::crossv_kfold()` will prepare data for cross validation in a nice way.	



# -----------------------------------------------------------------
# Classification
# -----------------------------------------------------------------
#' So far we've looked at models which predict a continuous response variable. There are many related models which predict categorical outcomes, such as whether an email is spam or not, or which digit a handwritten number is. We'll take a brief look at example of this: logistic regression.	

#' Since the function stays between zero and one, it can be interpreted as a mapping from predictor values to a probability of being in one of two classes.	

#' In this example, we will be working with the Titanic data where the survival status of passengers, as well as other information such as class, demographics, and fare are provided.

#' Let's read in the Titanic data
titanic <- read.csv('../data/titanic.csv', stringsAsFactors = F)

#' check data
str(titanic)


# ----------------------------------------------
# EXERCISE 4: EXPLORE ON YOUR OWN
# ----------------------------------------------
# ----------------------------------------------
# SOLUTION
# ----------------------------------------------

#' First we'll look at the relationship between age & survival
titanic %>% 
  ggplot() + 
  aes(Age, fill = factor(Survived)) +
  geom_histogram() + 
  facet_grid(.~Sex)

#' Now we look at it against Sibling/Spouse
titanic %>% 
  ggplot() +
  aes(x = SibSp, fill = factor(Survived)) + 
  geom_bar(stat='count', position='dodge') +
  scale_x_continuous(breaks=c(1:11))

#' Pclass and sex on survival probability
titanic %>% group_by(Sex, Pclass) %>% 
  summarize(Surv_prob = sum(Survived)/n()) %>%
  ggplot() +
  aes(x=Pclass, y=Sex, fill=Surv_prob) +
  geom_tile()

#' Let's do some simple feature engineering. 
#' Others have done sophisticated things with names, etc...
titanic <-titanic %>% 
  mutate(withFamily = ifelse(SibSp > 0 | Parch > 0, 1, 0),
         AgeGroup = ifelse(is.na(Age), "NA", 
                    ifelse(Age < 18, "Young", 
                    ifelse(Age < 45, "Mid-aged", "Old"))),
         AgeGroup = factor(AgeGroup, levels=c("NA", "Young", "Mid-aged", "Old")))


### Parititon to train and test
set.seed(1)
titanic_part <- titanic %>%
  resample_partition(c(test = 0.3, train = 0.7))

#' Run logistic regression on Sex, Sibling/Spouse, Parent/children, Pclass,and age

#' Instead of the `lm()` function, we'll now use `glm()`, but the syntax is almost exactly the same:	

l.glm <-glm(Survived ~ Sex + Pclass + SibSp + Parch + withFamily + AgeGroup,family="binomial",data=titanic_part$train)	
summary(l.glm)	

#' Make predictions:
l.pred <-predict(l.glm,newdata=titanic_part$test,type="response")

#' In the meantime, we can explore out-of-sample performance. We can compute the accuracy and AUC.
test_survived <- as.data.frame(titanic_part$test)$Survived
conf_table <- table(l.pred > 0.5, test_survived)
sum(diag(conf_table))/sum(conf_table)

#' The `ROCR` package is one implementation that allows us to plot ROC curves and calculate AuC. Here's an example (make sure to install the packages first using `install.packages("ROCR"))`:	

library(ROCR)	
pred_obj <-prediction(l.pred,test_survived) # Creating a prediction object for ROCR	
perf <-performance(pred_obj,'tpr','fpr')	
plot(perf, colorize = T)      # ROC curve	
performance(pred_obj,'auc')   # AUC - a scalar measure of performance	
	


#' As you can see, the `performance()` function in the `ROCR` package is versatile and allows you to calculate and plot a bunch of different performance metrics.	

#' In our case, this model gives an AUC of 0.81. The worst possible is 0.5 - random guessing. We're definitely better than random here, and could likely improve by adding more predictors.	

#' We've covered basic logistic regression, but just as with linear regression there are many, many extensions. For example, we could add higher-order predictor terms via splines. We could also do LASSO logistic regression if we wanted to use many predictors, using the `glmnet` package.	

